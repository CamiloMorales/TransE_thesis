{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRANSLATIONS EMBEDDINGS:\n",
    "\n",
    "1- First the creation of the data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of only_left/shared/only_right entities:  48 / 14786 / 117\n",
      "Number of relations:  1345\n",
      "train\n",
      "valid\n",
      "test\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import cPickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Put the freebase15k data absolute path here\n",
    "datapath = '/home/camilo/IPython_notebooks/TransE_embeddings/Data_FB15k/FB15k/'\n",
    "assert datapath is not None\n",
    "\n",
    "if 'data' not in os.listdir('../'):\n",
    "    os.mkdir('../data')\n",
    "\n",
    "def parseline(line):\n",
    "    lhs, rel, rhs = line.split('\\t')\n",
    "    lhs = lhs.split(' ')\n",
    "    rhs = rhs.split(' ')\n",
    "    rel = rel.split(' ')\n",
    "    return lhs, rel, rhs\n",
    "\n",
    "#################################################\n",
    "### Creation of the entities/indices dictionnaries\n",
    "\n",
    "np.random.seed(753)\n",
    "\n",
    "entleftlist = []\n",
    "entrightlist = []\n",
    "rellist = []\n",
    "\n",
    "for datatyp in ['train']:\n",
    "    f = open(datapath + 'freebase_mtr100_mte100-%s.txt' % datatyp, 'r')\n",
    "    dat = f.readlines()\n",
    "    f.close()\n",
    "    for i in dat:\n",
    "        lhs, rel, rhs = parseline(i[:-1])\n",
    "        entleftlist += [lhs[0]]\n",
    "        entrightlist += [rhs[0]]\n",
    "        rellist += [rel[0]]\n",
    "\n",
    "entleftset = np.sort(list(set(entleftlist) - set(entrightlist)))\n",
    "entsharedset = np.sort(list(set(entleftlist) & set(entrightlist)))\n",
    "entrightset = np.sort(list(set(entrightlist) - set(entleftlist)))\n",
    "relset = np.sort(list(set(rellist)))\n",
    "\n",
    "entity2idx = {}\n",
    "idx2entity = {}\n",
    "\n",
    "\n",
    "# we keep the entities specific to one side of the triplets contiguous\n",
    "idx = 0\n",
    "for i in entrightset:\n",
    "    entity2idx[i] = idx\n",
    "    idx2entity[idx] = i\n",
    "    idx += 1\n",
    "nbright = idx\n",
    "for i in entsharedset:\n",
    "    entity2idx[i] = idx\n",
    "    idx2entity[idx] = i\n",
    "    idx += 1\n",
    "nbshared = idx - nbright\n",
    "for i in entleftset:\n",
    "    entity2idx[i] = idx\n",
    "    idx2entity[idx] = i\n",
    "    idx += 1\n",
    "nbleft = idx - (nbshared + nbright)\n",
    "\n",
    "print \"# of only_left/shared/only_right entities: \", nbleft, '/', nbshared, '/', nbright\n",
    "# add relations at the end of the dictionary\n",
    "for i in relset:\n",
    "    entity2idx[i] = idx\n",
    "    idx2entity[idx] = i\n",
    "    idx += 1\n",
    "nbrel = idx - (nbright + nbshared + nbleft)\n",
    "print \"Number of relations: \", nbrel\n",
    "\n",
    "f = open('../data/FB15k_entity2idx.pkl', 'w')\n",
    "g = open('../data/FB15k_idx2entity.pkl', 'w')\n",
    "cPickle.dump(entity2idx, f, -1)\n",
    "cPickle.dump(idx2entity, g, -1)\n",
    "f.close()\n",
    "g.close()\n",
    "\n",
    "#################################################\n",
    "### Creation of the dataset files\n",
    "\n",
    "unseen_ents=[]\n",
    "remove_tst_ex=[]\n",
    "\n",
    "for datatyp in ['train', 'valid', 'test']:\n",
    "    print datatyp\n",
    "    f = open(datapath + 'freebase_mtr100_mte100-%s.txt' % datatyp, 'r')\n",
    "    dat = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    # Declare the dataset variables\n",
    "    inpl = sp.lil_matrix((np.max(entity2idx.values()) + 1, len(dat)),\n",
    "            dtype='float32')\n",
    "    inpr = sp.lil_matrix((np.max(entity2idx.values()) + 1, len(dat)),\n",
    "            dtype='float32')\n",
    "    inpo = sp.lil_matrix((np.max(entity2idx.values()) + 1, len(dat)),\n",
    "            dtype='float32')\n",
    "    # Fill the sparse matrices\n",
    "    ct = 0\n",
    "    for i in dat:\n",
    "        lhs, rel, rhs = parseline(i[:-1])\n",
    "        if lhs[0] in entity2idx and rhs[0] in entity2idx and rel[0] in entity2idx: \n",
    "            inpl[entity2idx[lhs[0]], ct] = 1\n",
    "            inpr[entity2idx[rhs[0]], ct] = 1\n",
    "            inpo[entity2idx[rel[0]], ct] = 1\n",
    "            ct += 1\n",
    "        else:\n",
    "            if lhs[0] in entity2idx:\n",
    "                unseen_ents+=[lhs[0]]\n",
    "            if rel[0] in entity2idx:\n",
    "                unseen_ents+=[rel[0]]\n",
    "            if rhs[0] in entity2idx:\n",
    "                unseen_ents+=[rhs[0]]\n",
    "            remove_tst_ex+=[i[:-1]]\n",
    "\n",
    "    # Save the datasets\n",
    "    if 'data' not in os.listdir('../'):\n",
    "        os.mkdir('../data')\n",
    "    f = open('../data/FB15k-%s-lhs.pkl' % datatyp, 'w')\n",
    "    g = open('../data/FB15k-%s-rhs.pkl' % datatyp, 'w')\n",
    "    h = open('../data/FB15k-%s-rel.pkl' % datatyp, 'w')\n",
    "    cPickle.dump(inpl.tocsr(), f, -1)\n",
    "    cPickle.dump(inpr.tocsr(), g, -1)\n",
    "    cPickle.dump(inpo.tocsr(), h, -1)\n",
    "    f.close()\n",
    "    g.close()\n",
    "    h.close()\n",
    "\n",
    "unseen_ents=list(set(unseen_ents))\n",
    "print len(unseen_ents)\n",
    "remove_tst_ex=list(set(remove_tst_ex))\n",
    "print len(remove_tst_ex)\n",
    "\n",
    "for i in remove_tst_ex:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Once byte-stream files (Entities/relations dictionaries) are created, and the initialization of their embeddings (represented as sparse matrices), we execute Stochastic gradient descend to \"get\" (its called \"learn\" when obtained through Gradient descend?) the embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 760M (CNMeM is enabled with initial size: 70.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import cPickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import theano\n",
    "import theano.sparse as S\n",
    "import theano.tensor as T\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we define the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath='data/'\n",
    "dataset='FB15k'\n",
    "Nent=16296\n",
    "rhoE=1\n",
    "rhoL=5\n",
    "Nsyn=14951\n",
    "Nrel=1345\n",
    "loadmodel=False\n",
    "loademb=False\n",
    "op='Unstructured'\n",
    "simfn='Dot'\n",
    "ndim=50\n",
    "nhid=50\n",
    "marge=1.\n",
    "lremb=0.1\n",
    "lrparam=1.\n",
    "nbatches=100\n",
    "totepochs=2000\n",
    "test_all=1\n",
    "neval=50\n",
    "seed=123\n",
    "savepath='.'\n",
    "loadmodelBi=False\n",
    "loadmodelTri=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then the specific parameters for TransE on the FB15k dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "op='TransE'\n",
    "simfn='L2'\n",
    "ndim=50\n",
    "nhid=50\n",
    "marge=0.5\n",
    "lremb=0.01\n",
    "lrparam=0.01\n",
    "nbatches=100\n",
    "totepochs=500\n",
    "test_all=10\n",
    "neval=1000\n",
    "savepath='FB15k_TransE'\n",
    "datapath='../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DD Class definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DD(dict):\n",
    "    \"\"\"This class is only used to replace a state variable of Jobman\"\"\"\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr == '__getstate__':\n",
    "            return super(DD, self).__getstate__\n",
    "        elif attr == '__setstate__':\n",
    "            return super(DD, self).__setstate__\n",
    "        elif attr == '__slots__':\n",
    "            return super(DD, self).__slots__\n",
    "        return self[attr]\n",
    "\n",
    "    def __setattr__(self, attr, value):\n",
    "        assert attr not in ('__getstate__', '__setstate__', '__slots__')\n",
    "        self[attr] = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'DD%s' % dict(self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        z = DD()\n",
    "        for k, kv in self.iteritems():\n",
    "            z[k] = copy.deepcopy(kv, memo)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Argument of the experiment script\n",
    "state = DD()\n",
    "state.datapath = datapath\n",
    "state.dataset = dataset\n",
    "state.Nent = Nent\n",
    "state.Nsyn = Nsyn\n",
    "state.Nrel = Nrel\n",
    "state.loadmodel = loadmodel\n",
    "state.loadmodelBi = loadmodelBi\n",
    "state.loadmodelTri = loadmodelTri\n",
    "state.loademb = loademb\n",
    "state.op = op\n",
    "state.simfn = simfn\n",
    "state.ndim = ndim\n",
    "state.nhid = nhid\n",
    "state.marge = marge\n",
    "state.rhoE = rhoE\n",
    "state.rhoL = rhoL\n",
    "state.lremb = lremb\n",
    "state.lrparam = lrparam\n",
    "state.nbatches = nbatches\n",
    "state.totepochs = totepochs\n",
    "state.test_all = test_all\n",
    "state.neval = neval\n",
    "state.seed = seed\n",
    "state.savepath = savepath\n",
    "\n",
    "if not os.path.isdir(state.savepath):\n",
    "        os.mkdir(state.savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Channel(object):\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "        f = open(self.state.savepath + '/orig_state.pkl', 'w')\n",
    "        cPickle.dump(self.state, f, -1)\n",
    "        f.close()\n",
    "        self.COMPLETE = 1\n",
    "\n",
    "    def save(self):\n",
    "        f = open(self.state.savepath + '/current_state.pkl', 'w')\n",
    "        cPickle.dump(self.state, f, -1)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "channel = Channel(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DD{'ndim': 50, 'test_all': 10, 'loadmodelBi': False, 'loadmodelTri': False, 'nhid': 50, 'lremb': 0.01, 'savepath': 'FB15k_TransE', 'seed': 123, 'marge': 0.5, 'simfn': 'L2', 'neval': 1000, 'dataset': 'FB15k', 'nbatches': 100, 'lrparam': 0.01, 'loademb': False, 'datapath': '../data/', 'Nrel': 1345, 'totepochs': 500, 'rhoL': 5, 'Nent': 16296, 'Nsyn': 14951, 'loadmodel': False, 'rhoE': 1, 'op': 'TransE'}\n"
     ]
    }
   ],
   "source": [
    "# FB15kexp(state, channel)\n",
    "\n",
    "# Show experiment parameters\n",
    "print >> sys.stderr, state\n",
    "np.random.seed(state.seed)\n",
    "\n",
    "# Experiment folder\n",
    "if hasattr(channel, 'remote_path'):\n",
    "    state.savepath = channel.remote_path + '/'\n",
    "elif hasattr(channel, 'path'):\n",
    "    state.savepath = channel.path + '/'\n",
    "else:\n",
    "    if not os.path.isdir(state.savepath):\n",
    "        os.mkdir(state.savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Necesary helping functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    return scipy.sparse.csr_matrix(cPickle.load(open(path)), dtype=theano.config.floatX) #Compressed Sparse Row matrix\n",
    "\n",
    "def convert2idx(spmat):\n",
    "    rows, cols = spmat.nonzero()\n",
    "    return rows[np.argsort(cols)]\n",
    "\n",
    "\n",
    "class LayerTrans(object):\n",
    "    \"\"\"\n",
    "    Class for a layer with two input vectors that performs the sum of \n",
    "    of the 'left member' and 'right member'i.e. translating x by y.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "        self.params = []\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        return x+y\n",
    "\n",
    "    \n",
    "class Unstructured(object):\n",
    "    \"\"\"\n",
    "    Class for a layer with two input vectors that performs the linear operator\n",
    "    of the 'left member'.\n",
    "\n",
    "    :note: The 'right' member is the relation, therefore this class allows to\n",
    "    define an unstructured layer (no effect of the relation) in the same\n",
    "    framework.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "        self.params = []\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Positives\n",
    "trainl = load_file(state.datapath + state.dataset + '-train-lhs.pkl')\n",
    "trainr = load_file(state.datapath + state.dataset + '-train-rhs.pkl')\n",
    "traino = load_file(state.datapath + state.dataset + '-train-rel.pkl')\n",
    "if state.op == 'SE' or state.op == 'TransE':\n",
    "    traino = traino[-state.Nrel:, :]\n",
    "#elif state.op =='Bi' or state.op == 'Tri'or state.op == 'TATEC':\n",
    "#    trainl = trainl[:state.Nsyn, :]\n",
    "#    trainr = trainr[:state.Nsyn, :]\n",
    "#    traino = traino[-state.Nrel:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Valid set\n",
    "validl = load_file(state.datapath + state.dataset + '-valid-lhs.pkl')\n",
    "validr = load_file(state.datapath + state.dataset + '-valid-rhs.pkl')\n",
    "valido = load_file(state.datapath + state.dataset + '-valid-rel.pkl')\n",
    "if state.op == 'SE' or state.op == 'TransE':\n",
    "    valido = valido[-state.Nrel:, :]\n",
    "#elif state.op =='Bi' or state.op == 'Tri'or state.op == 'TATEC':\n",
    "#    validl = validl[:state.Nsyn, :]\n",
    "#    validr = validr[:state.Nsyn, :]\n",
    "#    valido = valido[-state.Nrel:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Test set\n",
    "testl = load_file(state.datapath + state.dataset + '-test-lhs.pkl')\n",
    "testr = load_file(state.datapath + state.dataset + '-test-rhs.pkl')\n",
    "testo = load_file(state.datapath + state.dataset + '-test-rel.pkl')\n",
    "if state.op == 'SE' or state.op == 'TransE':\n",
    "    testo = testo[-state.Nrel:, :]\n",
    "#elif state.op =='Bi' or state.op == 'Tri'or state.op == 'TATEC':\n",
    "#    testl = testl[:state.Nsyn, :]\n",
    "#    testr = testr[:state.Nsyn, :]\n",
    "#    testo = testo[-state.Nrel:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Index conversion\n",
    "trainlidx = convert2idx(trainl)[:state.neval]\n",
    "trainridx = convert2idx(trainr)[:state.neval]\n",
    "trainoidx = convert2idx(traino)[:state.neval]\n",
    "\n",
    "validlidx = convert2idx(validl)[:state.neval]\n",
    "validridx = convert2idx(validr)[:state.neval]\n",
    "validoidx = convert2idx(valido)[:state.neval]\n",
    "\n",
    "testlidx = convert2idx(testl)[:state.neval]\n",
    "testridx = convert2idx(testr)[:state.neval]\n",
    "testoidx = convert2idx(testo)[:state.neval]\n",
    "\n",
    "idxl = convert2idx(trainl)\n",
    "idxr = convert2idx(trainr)\n",
    "idxo = convert2idx(traino)\n",
    "\n",
    "idxtl = convert2idx(testl)\n",
    "idxtr = convert2idx(testr)\n",
    "idxto = convert2idx(testo)\n",
    "\n",
    "idxvl = convert2idx(validl)\n",
    "idxvr = convert2idx(validr)\n",
    "idxvo = convert2idx(valido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_triples=np.concatenate([idxtl,idxvl,idxl,idxto,idxvo,idxo,idxtr,idxvr,idxr]).reshape(3,idxtl.shape[0]+idxvl.shape[0]+idxl.shape[0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embeddings class -----------------------------------------------------------\n",
    "class Embeddings(object):\n",
    "    \"\"\"Class for the embeddings matrix.\"\"\"\n",
    "\n",
    "    def __init__(self, rng, N, D, tag=''):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        :param rng: numpy.random module for number generation.\n",
    "        :param N: number of entities, relations or both.\n",
    "        :param D: dimension of the embeddings.\n",
    "        :param tag: name of the embeddings for parameter declaration.\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        wbound = np.sqrt(6. / D)\n",
    "        W_values = rng.uniform(low=-wbound, high=wbound, size=(D, N))\n",
    "        W_values = W_values / np.sqrt(np.sum(W_values ** 2, axis=0))\n",
    "        W_values = np.asarray(W_values, dtype=theano.config.floatX)\n",
    "        self.E = theano.shared(value=W_values, name='E' + tag)\n",
    "        # Define a normalization function with respect to the L_2 norm of the\n",
    "        # embedding vectors.\n",
    "        self.updates = OrderedDict({self.E: self.E / T.sqrt(T.sum(self.E ** 2, axis=0))})\n",
    "        self.normalize = theano.function([], [], updates=self.updates)\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "# Model declarationpp state\n",
    "#if state.op == 'TransE':\n",
    "leftop  = LayerTrans();#Tracer()() #this one triggers the debugger\n",
    "rightop = Unstructured()\n",
    "    \n",
    "# embeddings\n",
    "embeddings = Embeddings(np.random, state.Nent, state.ndim, 'emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if state.op == 'TransE' and type(embeddings) is not list:\n",
    "relationVec = Embeddings(np.random, state.Nrel, state.ndim, 'relvec')\n",
    "embeddings = [embeddings, relationVec, relationVec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L2sim(left, right):\n",
    "    return - T.sqrt(T.sum(T.sqr(left - right), axis=1))\n",
    "\n",
    "# Cost ------------------------------------------------------------------------\n",
    "def margincost(pos, neg, marge=1.0):\n",
    "    out = neg - pos + marge\n",
    "    return T.sum(out * (out > 0)), out > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simfn = eval(state.simfn + 'sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Utilitary function to parse the embeddings parameter in a normalized way\n",
    "    for the Structured Embedding [Bordes et al., AAAI 2011] and the Semantic\n",
    "    Matching Energy [Bordes et al., AISTATS 2012] models.\n",
    "    \"\"\"\n",
    "    if type(embeddings) == list:\n",
    "        embedding = embeddings[0]\n",
    "        relationl = embeddings[1]\n",
    "        relationr = embeddings[2]\n",
    "    else:\n",
    "        embedding = embeddings\n",
    "        relationl = embeddings\n",
    "        relationr = embeddings\n",
    "    return embedding, relationl, relationr\n",
    "\n",
    "def TrainFn1Member(fnsim, embeddings, leftop, rightop, marge=1.0, rel=True):\n",
    "    \"\"\"\n",
    "    This function returns a theano function to perform a training iteration,\n",
    "    contrasting positive and negative triplets. members are given as sparse\n",
    "    matrices. For one positive triplet there are two or three (if rel == True)\n",
    "    negative triplets. To create a negative triplet we replace only one member\n",
    "    at a time.\n",
    "\n",
    "    :param fnsim: similarity function (on theano variables).\n",
    "    :param embeddings: an embeddings instance.\n",
    "    :param leftop: class for the 'left' operator.\n",
    "    :param rightop: class for the 'right' operator.\n",
    "    :param marge: marge for the cost function.\n",
    "    :param rel: boolean, if true we also contrast w.r.t. a negative relation\n",
    "                member.\n",
    "    \"\"\"\n",
    "    embedding, relationl, relationr = parse_embeddings(embeddings)\n",
    "\n",
    "    # Inputs\n",
    "    inpr = S.csr_matrix()\n",
    "    inpl = S.csr_matrix()\n",
    "    inpo = S.csr_matrix()\n",
    "    inpln = S.csr_matrix()\n",
    "    inprn = S.csr_matrix()\n",
    "    lrparams = T.scalar('lrparams')\n",
    "    lrembeddings = T.scalar('lrembeddings')\n",
    "\n",
    "    # Graph\n",
    "    lhs = S.dot(embedding.E, inpl).T\n",
    "    rhs = S.dot(embedding.E, inpr).T\n",
    "    rell = S.dot(relationl.E, inpo).T\n",
    "    relr = S.dot(relationr.E, inpo).T\n",
    "    lhsn = S.dot(embedding.E, inpln).T\n",
    "    rhsn = S.dot(embedding.E, inprn).T\n",
    "    simi = fnsim(leftop(lhs, rell), rightop(rhs, relr))\n",
    "    # Negative 'left' member\n",
    "    similn = fnsim(leftop(lhsn, rell), rightop(rhs, relr))\n",
    "    # Negative 'right' member\n",
    "    simirn = fnsim(leftop(lhs, rell), rightop(rhsn, relr))\n",
    "    costl, outl = margincost(simi, similn, marge)\n",
    "    costr, outr = margincost(simi, simirn, marge)\n",
    "    cost = costl + costr\n",
    "    out = T.concatenate([outl, outr])\n",
    "    # List of inputs of the function\n",
    "    list_in = [lrembeddings, lrparams,\n",
    "            inpl, inpr, inpo, inpln, inprn]\n",
    "    if rel:\n",
    "        # If rel is True, we also consider a negative relation member\n",
    "        inpon = S.csr_matrix()\n",
    "        relln = S.dot(relationl.E, inpon).T\n",
    "        relrn = S.dot(relationr.E, inpon).T\n",
    "        simion = fnsim(leftop(lhs, relln), rightop(rhs, relrn))\n",
    "        costo, outo = margincost(simi, simion, marge)\n",
    "        cost += costo\n",
    "        out = T.concatenate([out, outo])\n",
    "        list_in += [inpon]\n",
    "\n",
    "    if hasattr(fnsim, 'params'):\n",
    "        # If the similarity function has some parameters, we update them too.\n",
    "        gradientsparams = T.grad(cost,\n",
    "            leftop.params + rightop.params + fnsim.params)\n",
    "        updates = OrderedDict((i, i - lrparams * j) for i, j in zip(\n",
    "            leftop.params + rightop.params + fnsim.params, gradientsparams))\n",
    "    else:\n",
    "        gradientsparams = T.grad(cost, leftop.params + rightop.params)\n",
    "        updates = OrderedDict((i, i - lrparams * j) for i, j in zip(\n",
    "            leftop.params + rightop.params, gradientsparams))\n",
    "    gradients_embedding = T.grad(cost, embedding.E)\n",
    "    newE = embedding.E - lrembeddings * gradients_embedding\n",
    "    updates.update({embedding.E: newE})\n",
    "    if type(embeddings) == list:\n",
    "        # If there are different embeddings for the relation member.\n",
    "        gradients_embedding = T.grad(cost, relationl.E)\n",
    "        newE = relationl.E - lrparams * gradients_embedding\n",
    "        updates.update({relationl.E: newE})\n",
    "        gradients_embedding = T.grad(cost, relationr.E)\n",
    "        newE = relationr.E - lrparams * gradients_embedding\n",
    "        updates.update({relationr.E: newE})\n",
    "    \"\"\"\n",
    "    Theano function inputs.\n",
    "    :input lrembeddings: learning rate for the embeddings.\n",
    "    :input lrparams: learning rate for the parameters.\n",
    "    :input inpl: sparse csr matrix representing the indexes of the positive\n",
    "                 triplet 'left' member, shape=(#examples,N [Embeddings]).\n",
    "    :input inpr: sparse csr matrix representing the indexes of the positive\n",
    "                 triplet 'right' member, shape=(#examples,N [Embeddings]).\n",
    "    :input inpo: sparse csr matrix representing the indexes of the positive\n",
    "                 triplet relation member, shape=(#examples,N [Embeddings]).\n",
    "    :input inpln: sparse csr matrix representing the indexes of the negative\n",
    "                  triplet 'left' member, shape=(#examples,N [Embeddings]).\n",
    "    :input inprn: sparse csr matrix representing the indexes of the negative\n",
    "                  triplet 'right' member, shape=(#examples,N [Embeddings]).\n",
    "    :opt input inpon: sparse csr matrix representing the indexes of the\n",
    "                      negative triplet relation member, shape=(#examples,N\n",
    "                      [Embeddings]).\n",
    "\n",
    "    Theano function output.\n",
    "    :output mean(cost): average cost.\n",
    "    :output mean(out): ratio of examples for which the margin is violated,\n",
    "                       i.e. for which an update occurs.\n",
    "    \"\"\"\n",
    "    return theano.function(list_in, [T.mean(cost), T.mean(out)],\n",
    "            updates=updates, on_unused_input='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RankLeftFnIdx(fnsim, embeddings, leftop, rightop, subtensorspec=None):\n",
    "    \"\"\"\n",
    "    This function returns a Theano function to measure the similarity score of\n",
    "    all 'left' entities given couples of relation and 'right' entities (as\n",
    "    index values).\n",
    "\n",
    "    :param fnsim: similarity function (on Theano variables).\n",
    "    :param embeddings: an Embeddings instance.\n",
    "    :param leftop: class for the 'left' operator.\n",
    "    :param rightop: class for the 'right' operator.\n",
    "    :param subtensorspec: only measure the similarity score for the entities\n",
    "                          corresponding to the first subtensorspec (int)\n",
    "                          entities of the embedding matrix (default None: all\n",
    "                          entities).\n",
    "    \"\"\"\n",
    "    embedding, relationl, relationr = parse_embeddings(embeddings)\n",
    "\n",
    "    # Inputs\n",
    "    idxr = T.iscalar('idxr')\n",
    "    idxo = T.iscalar('idxo')\n",
    "    # Graph\n",
    "    if subtensorspec is not None:\n",
    "        # We compute the score only for a subset of entities\n",
    "        lhs = (embedding.E[:, :subtensorspec]).T\n",
    "    else:\n",
    "        lhs = embedding.E.T\n",
    "    rhs = (embedding.E[:, idxr]).reshape((1, embedding.D))\n",
    "    rell = (relationl.E[:, idxo]).reshape((1, relationl.D))\n",
    "    relr = (relationr.E[:, idxo]).reshape((1, relationr.D))\n",
    "    tmp = rightop(rhs, relr)\n",
    "    simi = fnsim(leftop(lhs, rell), tmp.reshape((1, tmp.shape[1])))\n",
    "    \"\"\"\n",
    "    Theano function inputs.\n",
    "    :input idxr: index value of the 'right' member.\n",
    "    :input idxo: index value of the relation member.\n",
    "\n",
    "    Theano function output.\n",
    "    :output simi: vector of score values.\n",
    "    \"\"\"\n",
    "    return theano.function([idxr, idxo], [simi], on_unused_input='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RankRightFnIdx(fnsim, embeddings, leftop, rightop, subtensorspec=None):\n",
    "    \"\"\"\n",
    "    This function returns a Theano function to measure the similarity score of\n",
    "    all 'right' entities given couples of relation and 'left' entities (as\n",
    "    index values).\n",
    "\n",
    "    :param fnsim: similarity function (on Theano variables).\n",
    "    :param embeddings: an Embeddings instance.\n",
    "    :param leftop: class for the 'left' operator.\n",
    "    :param rightop: class for the 'right' operator.\n",
    "    :param subtensorspec: only measure the similarity score for the entities\n",
    "                          corresponding to the first subtensorspec (int)\n",
    "                          entities of the embedding matrix (default None: all\n",
    "                          entities).\n",
    "    \"\"\"\n",
    "    embedding, relationl, relationr = parse_embeddings(embeddings)\n",
    "\n",
    "    # Inputs\n",
    "    idxl = T.iscalar('idxl')\n",
    "    idxo = T.iscalar('idxo')\n",
    "    # Graph\n",
    "    lhs = (embedding.E[:, idxl]).reshape((1, embedding.D))\n",
    "    if subtensorspec is not None:\n",
    "        # We compute the score only for a subset of entities\n",
    "        rhs = (embedding.E[:, :subtensorspec]).T\n",
    "    else:\n",
    "        rhs = embedding.E.T\n",
    "    rell = (relationl.E[:, idxo]).reshape((1, relationl.D))\n",
    "    relr = (relationr.E[:, idxo]).reshape((1, relationr.D))\n",
    "    tmp = leftop(lhs, rell)\n",
    "    simi = fnsim(tmp.reshape((1, tmp.shape[1])), rightop(rhs, relr))\n",
    "    \"\"\"\n",
    "    Theano function inputs.\n",
    "    :input idxl: index value of the 'left' member.\n",
    "    :input idxo: index value of the relation member.\n",
    "\n",
    "    Theano function output.\n",
    "    :output simi: vector of score values.\n",
    "    \"\"\"\n",
    "    return theano.function([idxl, idxo], [simi], on_unused_input='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function compilation\n",
    "trainfunc = TrainFn1Member(simfn, embeddings, leftop, rightop, marge=state.marge, rel=False)\n",
    "ranklfunc = RankLeftFnIdx(simfn, embeddings, leftop, rightop, subtensorspec=state.Nsyn)\n",
    "rankrfunc = RankRightFnIdx(simfn, embeddings, leftop, rightop, subtensorspec=state.Nsyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = []\n",
    "outb = []\n",
    "state.bestvalid = -1\n",
    "\n",
    "batchsize = trainl.shape[1] / state.nbatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_random_mat(shape, listidx=None):\n",
    "    \"\"\"\n",
    "    This function create a random sparse index matrix with a given shape. It\n",
    "    is useful to create negative triplets.\n",
    "\n",
    "    :param shape: shape of the desired sparse matrix.\n",
    "    :param listidx: list of index to sample from (default None: it samples from\n",
    "                    all shape[0] indexes).\n",
    "\n",
    "    :note: if shape[1] > shape[0], it loops over the shape[0] indexes.\n",
    "    \"\"\"\n",
    "    if listidx is None:\n",
    "        listidx = np.arange(shape[0])\n",
    "    listidx = listidx[np.random.permutation(len(listidx))]\n",
    "    randommat = scipy.sparse.lil_matrix((shape[0], shape[1]),\n",
    "            dtype=theano.config.floatX)\n",
    "    idx_term = 0\n",
    "    for idx_ex in range(shape[1]):\n",
    "        if idx_term == len(listidx):\n",
    "            idx_term = 0\n",
    "        randommat[listidx[idx_term], idx_ex] = 1\n",
    "        idx_term += 1\n",
    "    return randommat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FilteredRankingScoreIdx(sl, sr, idxl, idxr, idxo, true_triples):\n",
    "    \"\"\"\n",
    "    This function computes the rank list of the lhs and rhs, over a list of\n",
    "    lhs, rhs and rel indexes.\n",
    "\n",
    "    :param sl: Theano function created with RankLeftFnIdx().\n",
    "    :param sr: Theano function created with RankRightFnIdx().\n",
    "    :param idxl: list of 'left' indices.\n",
    "    :param idxr: list of 'right' indices.\n",
    "    :param idxo: list of relation indices.\n",
    "    \"\"\"\n",
    "    errl = []\n",
    "    errr = []\n",
    "    for l, o, r in zip(idxl, idxo, idxr):\n",
    "        il=np.argwhere(true_triples[:,0]==l).reshape(-1,)\n",
    "        io=np.argwhere(true_triples[:,1]==o).reshape(-1,)\n",
    "        ir=np.argwhere(true_triples[:,2]==r).reshape(-1,)\n",
    " \n",
    "        inter_l = [i for i in ir if i in io]\n",
    "        rmv_idx_l = [true_triples[i,0] for i in inter_l if true_triples[i,0] != l]\n",
    "        scores_l = (sl(r, o)[0]).flatten()\n",
    "        scores_l[rmv_idx_l] = -np.inf\n",
    "        errl += [np.argsort(np.argsort(-scores_l)).flatten()[l] + 1]\n",
    "\n",
    "        inter_r = [i for i in il if i in io]\n",
    "        rmv_idx_r = [true_triples[i,2] for i in inter_r if true_triples[i,2] != r]\n",
    "        scores_r = (sr(l, o)[0]).flatten()\n",
    "        scores_r[rmv_idx_r] = -np.inf\n",
    "        errr += [np.argsort(np.argsort(-scores_r)).flatten()[r] + 1]\n",
    "    return errl, errr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-6c036dbe5313>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Negatives\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrainln\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_random_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNsyn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtrainrn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_random_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNsyn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-c8600d67c5ea>\u001b[0m in \u001b[0;36mcreate_random_mat\u001b[1;34m(shape, listidx)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0midx_term\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistidx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0midx_term\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mrandommat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlistidx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_term\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_ex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0midx_term\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrandommat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/lil.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, index, x)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;31m# with fancy indexing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             if ((isinstance(i, int) or isinstance(i, np.integer)) and\n\u001b[1;32m--> 276\u001b[1;33m                     (isinstance(j, int) or isinstance(j, np.integer))):\n\u001b[0m\u001b[0;32m    277\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print >> sys.stderr, \"BEGIN TRAINING\"\n",
    "timeref = time.time()\n",
    "for epoch_count in xrange(1, state.totepochs + 1):\n",
    "    # Shuffling\n",
    "    order = np.random.permutation(trainl.shape[1])\n",
    "    trainl = trainl[:, order]\n",
    "    trainr = trainr[:, order]\n",
    "    traino = traino[:, order]\n",
    "\n",
    "    # Negatives\n",
    "    trainln = create_random_mat(trainl.shape, np.arange(state.Nsyn))\n",
    "    trainrn = create_random_mat(trainr.shape, np.arange(state.Nsyn))\n",
    "\n",
    "    for i in range(state.nbatches):\n",
    "        tmpl = trainl[:, i * batchsize:(i + 1) * batchsize]\n",
    "        tmpr = trainr[:, i * batchsize:(i + 1) * batchsize]\n",
    "        tmpo = traino[:, i * batchsize:(i + 1) * batchsize]\n",
    "        tmpnl = trainln[:, i * batchsize:(i + 1) * batchsize]\n",
    "        tmpnr = trainrn[:, i * batchsize:(i + 1) * batchsize]\n",
    "        # training iteration\n",
    "        outtmp = trainfunc(state.lremb, state.lrparam,\n",
    "                tmpl, tmpr, tmpo, tmpnl, tmpnr)\n",
    "        out += [outtmp[0] / float(batchsize)]\n",
    "        outb += [outtmp[1]]\n",
    "        # embeddings normalization\n",
    "        if type(embeddings) is list and state.op == 'Bi':\n",
    "            auxE = embeddings[0].E.get_value()\n",
    "            idx=np.where(np.sqrt(np.sum(auxE ** 2, axis=0)) > state.rhoE)\n",
    "            auxE[:, idx] = (state.rhoE*auxE[:, idx]) / np.sqrt(np.sum(auxE[:, idx] ** 2, axis=0))\n",
    "            embeddings[0].E.set_value(auxE)\n",
    "        elif type(embeddings) is list and state.op == 'Tri':\n",
    "            auxE = embeddings[0].E.get_value()\n",
    "            idx=np.where(np.sqrt(np.sum(auxE ** 2, axis=0)) > state.rhoE)\n",
    "            auxE[:, idx] = (state.rhoE*auxE[:, idx]) / np.sqrt(np.sum(auxE[:, idx] ** 2, axis=0))\n",
    "            embeddings[0].E.set_value(auxE)\n",
    "            auxR = embeddings[1].E.get_value()\n",
    "            idx=np.where(np.sqrt(np.sum(auxR ** 2, axis=0)) > state.rhoL)\n",
    "            auxR[:, idx] = (state.rhoL*auxR[:, idx]) / np.sqrt(np.sum(auxR[:, idx] ** 2, axis=0))\n",
    "            embeddings[1].E.set_value(auxR)\n",
    "        elif type(embeddings) is list and state.op == 'TATEC':\n",
    "            auxEb = embeddings[0].E.get_value()\n",
    "            idxb=np.where(np.sqrt(np.sum(auxEb ** 2, axis=0)) > state.rhoE)\n",
    "            auxEb[:, idxb] = (state.rhoE*auxEb[:, idxb]) / np.sqrt(np.sum(auxEb[:, idxb] ** 2, axis=0))\n",
    "            embeddings[0].E.set_value(auxEb)\n",
    "            auxEt = embeddings[4].E.get_value()\n",
    "            idxt=np.where(np.sqrt(np.sum(auxEt ** 2, axis=0)) > state.rhoE)\n",
    "            auxEt[:, idxt] = (state.rhoE*auxEt[:, idxt]) / np.sqrt(np.sum(auxEt[:, idxt] ** 2, axis=0))\n",
    "            embeddings[4].E.set_value(auxEt)\n",
    "            auxR = embeddings[5].E.get_value()\n",
    "            idxr=np.where(np.sqrt(np.sum(auxR ** 2, axis=0)) > state.rhoL)\n",
    "            auxR[:, idxr] = (state.rhoL*auxR[:, idxr]) / np.sqrt(np.sum(auxR[:, idxr] ** 2, axis=0))\n",
    "            embeddings[5].E.set_value(auxR)\n",
    "        elif type(embeddings) is list:\n",
    "            embeddings[0].normalize()\n",
    "        else:\n",
    "            embeddings.normalize()\n",
    "\n",
    "    if (epoch_count % state.test_all) == 0:\n",
    "        # model evaluation\n",
    "        print >> sys.stderr, \"-- EPOCH %s (%s seconds per epoch):\" % (\n",
    "                epoch_count,\n",
    "                round(time.time() - timeref, 3) / float(state.test_all))\n",
    "        timeref = time.time()\n",
    "        print >> sys.stderr, \"COST >> %s +/- %s, %% updates: %s%%\" % (\n",
    "                round(np.mean(out), 4), round(np.std(out), 4),\n",
    "                round(np.mean(outb) * 100, 3))\n",
    "        out = []\n",
    "        outb = []\n",
    "        resvalid = FilteredRankingScoreIdx(ranklfunc, rankrfunc,\n",
    "                validlidx, validridx, validoidx, true_triples)\n",
    "        state.valid = np.mean(resvalid[0] + resvalid[1])\n",
    "        restrain = FilteredRankingScoreIdx(ranklfunc, rankrfunc,\n",
    "                trainlidx, trainridx, trainoidx, true_triples)\n",
    "        state.train = np.mean(restrain[0] + restrain[1])\n",
    "        print >> sys.stderr, \"\\tMEAN RANK >> valid: %s, train: %s\" % (\n",
    "                state.valid, state.train)\n",
    "        if state.bestvalid == -1 or state.valid < state.bestvalid:\n",
    "            restest = FilteredRankingScoreIdx(ranklfunc, rankrfunc,\n",
    "                    testlidx, testridx, testoidx, true_triples)\n",
    "            state.bestvalid = state.valid\n",
    "            state.besttrain = state.train\n",
    "            state.besttest = np.mean(restest[0] + restest[1])\n",
    "            state.bestepoch = epoch_count\n",
    "            # Save model best valid model\n",
    "            f = open(state.savepath + '/best_valid_model.pkl', 'w')\n",
    "            if state.op == 'TATEC':\n",
    "                cPickle.dump(embeddings, f, -1)\n",
    "                cPickle.dump(leftopbi, f, -1)\n",
    "                cPickle.dump(leftoptri, f, -1)\n",
    "                cPickle.dump(rightopbi, f, -1)\n",
    "                cPickle.dump(rightoptri, f, -1)\n",
    "            else:\n",
    "                cPickle.dump(embeddings, f, -1)\n",
    "                cPickle.dump(leftop, f, -1)\n",
    "                cPickle.dump(rightop, f, -1)\n",
    "                cPickle.dump(simfn, f, -1)\n",
    "            f.close()\n",
    "            print >> sys.stderr, \"\\t\\t##### NEW BEST VALID >> test: %s\" % (\n",
    "                    state.besttest)\n",
    "        # Save current model\n",
    "        f = open(state.savepath + '/current_model.pkl', 'w')\n",
    "        if state.op == 'TATEC':\n",
    "            cPickle.dump(embeddings, f, -1)\n",
    "            cPickle.dump(leftopbi, f, -1)\n",
    "            cPickle.dump(leftoptri, f, -1)\n",
    "            cPickle.dump(rightopbi, f, -1)\n",
    "            cPickle.dump(rightoptri, f, -1)\n",
    "        else:\n",
    "            cPickle.dump(embeddings, f, -1)\n",
    "            cPickle.dump(leftop, f, -1)\n",
    "            cPickle.dump(rightop, f, -1)\n",
    "            cPickle.dump(simfn, f, -1)\n",
    "        f.close()\n",
    "        state.nbepochs = epoch_count\n",
    "        print >> sys.stderr, \"\\t(the evaluation took %s seconds)\" % (\n",
    "            round(time.time() - timeref, 3))\n",
    "        timeref = time.time()\n",
    "        channel.save()\n",
    "#return channel.COMPLETE\n",
    "channel.COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING\n",
      "-- EPOCH 10 (10.9732 seconds per epoch):\n",
      "COST >> 0.1871 +/- 0.1833, % updates: 30.116%\n",
      "\tMEAN RANK >> valid: 273.1625, train: 193.3545\n",
      "\t\t##### NEW BEST VALID >> test: 272.7645\n",
      "\t(the evaluation took 61.822 seconds)\n",
      "-- EPOCH 20 (11.5837 seconds per epoch):\n",
      "COST >> 0.0636 +/- 0.0048, % updates: 12.44%\n",
      "\tMEAN RANK >> valid: 192.189, train: 128.6595\n",
      "\t\t##### NEW BEST VALID >> test: 192.237\n",
      "\t(the evaluation took 69.957 seconds)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-974af5e18617>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Negatives\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrainln\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_random_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNsyn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtrainrn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_random_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNsyn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-c8600d67c5ea>\u001b[0m in \u001b[0;36mcreate_random_mat\u001b[1;34m(shape, listidx)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0midx_term\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistidx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0midx_term\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mrandommat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlistidx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_term\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_ex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0midx_term\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrandommat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/lil.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, index, x)\u001b[0m\n\u001b[0;32m    280\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trying to assign a sequence to an item\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m                 _csparsetools.lil_insert(self.shape[0], self.shape[1],\n\u001b[1;32m--> 282\u001b[1;33m                                          self.rows, self.data, i, j, x)\n\u001b[0m\u001b[0;32m    283\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print >> sys.stderr, \"BEGIN TRAINING\"\n",
    "timeref = time.time()\n",
    "for epoch_count in xrange(1, state.totepochs + 1):\n",
    "    # Shuffling\n",
    "    order = np.random.permutation(trainl.shape[1])\n",
    "    trainl = trainl[:, order]\n",
    "    trainr = trainr[:, order]\n",
    "    traino = traino[:, order]\n",
    "\n",
    "    # Negatives\n",
    "    trainln = create_random_mat(trainl.shape, np.arange(state.Nsyn))\n",
    "    trainrn = create_random_mat(trainr.shape, np.arange(state.Nsyn))\n",
    "\n",
    "    for i in range(state.nbatches):\n",
    "        tmpl = trainl[:, i * batchsize:(i + 1) * batchsize]\n",
    "        tmpr = trainr[:, i * batchsize:(i + 1) * batchsize]\n",
    "        tmpo = traino[:, i * batchsize:(i + 1) * batchsize]\n",
    "        tmpnl = trainln[:, i * batchsize:(i + 1) * batchsize]\n",
    "        tmpnr = trainrn[:, i * batchsize:(i + 1) * batchsize]\n",
    "        # training iteration\n",
    "        outtmp = trainfunc(state.lremb, state.lrparam,\n",
    "                tmpl, tmpr, tmpo, tmpnl, tmpnr)\n",
    "        out += [outtmp[0] / float(batchsize)]\n",
    "        outb += [outtmp[1]]\n",
    "        \n",
    "        # embeddings normalization\n",
    "        embeddings[0].normalize()\n",
    "\n",
    "    if (epoch_count % state.test_all) == 0:\n",
    "        # model evaluation\n",
    "        print >> sys.stderr, \"-- EPOCH %s (%s seconds per epoch):\" % (\n",
    "                epoch_count,\n",
    "                round(time.time() - timeref, 3) / float(state.test_all))\n",
    "        timeref = time.time()\n",
    "        print >> sys.stderr, \"COST >> %s +/- %s, %% updates: %s%%\" % (\n",
    "                round(np.mean(out), 4), round(np.std(out), 4),\n",
    "                round(np.mean(outb) * 100, 3))\n",
    "        out = []\n",
    "        outb = []\n",
    "        resvalid = FilteredRankingScoreIdx(ranklfunc, rankrfunc, validlidx, validridx, validoidx, true_triples)\n",
    "        state.valid = np.mean(resvalid[0] + resvalid[1])\n",
    "        restrain = FilteredRankingScoreIdx(ranklfunc, rankrfunc, trainlidx, trainridx, trainoidx, true_triples)\n",
    "        state.train = np.mean(restrain[0] + restrain[1])\n",
    "        print >> sys.stderr, \"\\tMEAN RANK >> valid: %s, train: %s\" % (state.valid, state.train)\n",
    "        if state.bestvalid == -1 or state.valid < state.bestvalid:\n",
    "            restest = FilteredRankingScoreIdx(ranklfunc, rankrfunc,testlidx, testridx, testoidx, true_triples)\n",
    "            state.bestvalid = state.valid\n",
    "            state.besttrain = state.train\n",
    "            state.besttest = np.mean(restest[0] + restest[1])\n",
    "            state.bestepoch = epoch_count\n",
    "            # Save model best valid model\n",
    "            f = open(state.savepath + '/best_valid_model.pkl', 'w')\n",
    "\n",
    "            cPickle.dump(embeddings, f, -1)\n",
    "            cPickle.dump(leftop, f, -1)\n",
    "            cPickle.dump(rightop, f, -1)\n",
    "            cPickle.dump(simfn, f, -1)\n",
    "                \n",
    "            f.close()\n",
    "            print >> sys.stderr, \"\\t\\t##### NEW BEST VALID >> test: %s\" % (state.besttest)\n",
    "        \n",
    "        # Save current model\n",
    "        f = open(state.savepath + '/current_model.pkl', 'w')\n",
    "\n",
    "        cPickle.dump(embeddings, f, -1)\n",
    "        cPickle.dump(leftop, f, -1)\n",
    "        cPickle.dump(rightop, f, -1)\n",
    "        cPickle.dump(simfn, f, -1)\n",
    "            \n",
    "        f.close()\n",
    "        state.nbepochs = epoch_count\n",
    "        print >> sys.stderr, \"\\t(the evaluation took %s seconds)\" % (round(time.time() - timeref, 3))\n",
    "        timeref = time.time()\n",
    "        channel.save()\n",
    "#return channel.COMPLETE\n",
    "channel.COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
